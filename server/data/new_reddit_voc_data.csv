<<<<<<< HEAD
source,url,title,body,created_at,upvote
aws,https://www.reddit.com/r/aws/comments/1ayykyl/aws_transcribe_is_crazy_expensive_how_to_lower/,AWS Transcribe is crazy expensive. How to lower costs and keep comparable performance?,"Looking for information on AWS hosted alternatives, e.g. Whisper on EC2. Has anyone switched over to this? If so how low are you able to get your costs to transcribe audio and video with comparable quality?",1708792054.0,17
aws,https://www.reddit.com/r/aws/comments/1azggur/one_vs_more_vpn/,One vs more VPN,"We are debating if se should use one or more Customer Gateway/VPN connections with our Transit Gateway. To me it looks like it just increases how much resources we need to maintain(more in Aws and onprem both), without adding much resiliency.
=======
source,url,title,body,created_at,upvote
swiggy,https://www.reddit.com/r/aws/comments/1axx5c9/how_to_forward_authorization_header_using/,How to forward Authorization header using CloudFront Origin Request whitelist?,"I'm using AWS AppSync as origin (think of it as similar to API Gateway). I've disabled my CloudFront cache using using the [CachingDisabled](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-managed-cache-policies.html#managed-cache-policy-caching-disabled) managed policy.

I don't want to forward the `Host` header as it may [prevent AppSync form working](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-managed-origin-request-policies.html#managed-origin-request-policy-all-viewer-except-host-header), and I need to forward the `Authorization` header. In simple terms:

*  `Host`: Should not be forwarded
* `Authorization`: Should be forwarded

When I try to implement this strategy with my own Origin Request Policy [whitelist](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/cloudfront/create-origin-request-policy.html#:~:text=whitelist) I get an error:

>You can't use an origin request policy to forward the Authorization header. The header must be a part of the cache key to prevent the cache from satisfying unauthorized requests.

&#x200B;

[CloudFront Origin Request Policy doesn't allow to whitelist Authorization HTTP header](https://preview.redd.it/dn51y3td7bkc1.png?width=531&format=png&auto=webp&s=59a4448e5d29de3a71a8eb46bc7543dd7782b992)

What? But my cache is disabled by design.

Now when instead of a [whitelist](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/cloudfront/create-origin-request-policy.html#:~:text=whitelist) I use [allExcept](https://awscli.amazonaws.com/v2/documentation/api/latest/reference/cloudfront/create-origin-request-policy.html#:~:text=allExcept) and I explicitly block the `Host` header and implicitely forward  `Authorization` then everything works fine.

Why that? It doesn't make any sense, I should be able to achieve the same using a whitelist or a black list. I prefer the whitelist approach because it expresses more strict rules.

Is this a bug? if now, someone could explain me the rationale behind this?

PS: This is somewhat related to this SO question: [https://serverfault.com/questions/1053906/how-to-whitelist-authorization-header-in-cloudfront-custom-origin-request-policy](https://serverfault.com/questions/1053906/how-to-whitelist-authorization-header-in-cloudfront-custom-origin-request-policy)

&#x200B;",1708683425.0,3
swiggy,https://www.reddit.com/r/aws/comments/1axwufl/launch_template_that_always_uses_latest_image/,Launch template that always uses latest image ?,"Currently I have a launch template that uses the SSM parameter (
/aws/service/ami-amazon-linux-latest/al2023-ami-kernel-default-x86_64 ) as the image_id however this means  that I need to update the launch template each time (with my CI/CD). 


Is there a way to make a launch template that ""always takes the latest image"" without having to make a new launch template ?",1708682242.0,3
swiggy,https://www.reddit.com/r/aws/comments/1axvg7q/asynchronous_lambda_gotchas/,Asynchronous lambda gotchas?,"My team is planning to go all-in on async lambda. 

Our company already uses a bunch of lambda, but through sqs -> lambda for our data processing jobs, part of which we might change to async.

Wondering if people know of anything to look out for, like any weird behavior with it to keep track of",1708676349.0,4
swiggy,https://www.reddit.com/r/aws/comments/1axf0ll/failed_loop/,Failed Loop,"I interviewed for a senior TAM position and today I got the rejection, 2 days after the loop.
>>>>>>> main

As I can tell our current CGW/connection have never been down, did AWS ever have outtage id this service?
And if it did, it would impact us the same no matter if we had 1 or 4 CGWs as they would all be affected.",1708840108.0,1
aws,https://www.reddit.com/r/aws/comments/1azburu/new_accounts_under_organisation_are_not/,New accounts under organisation are not consolidating billing,"I have a personal AWS org. The management account is setup with billing and so I can create resources within it. Obviously, I don't want to use the management account for that - but every time I create a new account in the org it is considered ""NotSignedUp"", so accessing S3 will 404, etc. 

As I understand it, any new/invited account within an org now, by default, consolidates its billing under the management account?

Does anyone know why new accounts might be treated as NotSignedUp?",1708825514.0,2
aws,https://www.reddit.com/r/aws/comments/1azfkay/mqtt_connection_failure_with_aws_iot_core/,MQTT connection failure with AWS IoT Core,"I need some assistance with a frustrating issue I'm encountering while connecting my device to AWS IoT Core. I'm using the \`aws-iot-device-sdk-v2\` library for JS and keep hitting this error:

`Error: aws-c-io: AWS_IO_TLS_ERROR_NEGOTIATION_FAILURE, TLS (SSL) negotiation failed`

<<<<<<< HEAD
This happens when I try to connect using `newWebsocketMqttBuilderWithSigv4Auth`
=======
What to make of this? I really want to join Amazon because I am specialized in cloud computing. Should I keep refining my skills and try again in the future, or give up? I don't know what to fix exactly given the lack of feedback from the recruiter...",1708629617.0,17
swiggy,https://www.reddit.com/r/aws/comments/1axxsa2/creating_a_qc_ui_for_textract_quality_checking/,Creating a QC UI for Textract Quality Checking.," I am using Textract to extract tabular data. Unfortunately, Textract does not actually give out the correct OCR values most of the times, so I am using a Elastic Search to fit the values returned by Textract to my required domain. 

I also need to create an interface where a human reviewer can review the Image and the Textract output side by side. I need this to be a web interface. Any ideas how I can go about this ? I am only an intern at this place so really needing some help here. ",1708686013.0,1
swiggy,https://www.reddit.com/r/aws/comments/1axxs9v/creating_a_qc_ui_for_textract_quality_checking/,Creating a QC UI for Textract Quality Checking.," I am using Textract to extract tabular data. Unfortunately, Textract does not actually give out the correct OCR values most of the times, so I am using a Elastic Search to fit the values returned by Textract to my required domain. 

I also need to create an interface where a human reviewer can review the Image and the Textract output side by side. I need this to be a web interface. Any ideas how I can go about this ? I am only an intern at this place so really needing some help here. ",1708686013.0,1
swiggy,https://www.reddit.com/r/aws/comments/1axw0t8/aws_cloudformation_create_secret_with_variables/,aws cloud-formation create secret with variables,"I am trying to take username and password from user as input parameters and create the secret in secret manager. Using ${Username}  
 doesnt seem to replace and could not find a documentation on how to go about this. Is it even supported? if not, any recommended workarounds?

    AWSTemplateFormatVersion: '2010-09-09'
    Resources:
      MySecret:
        Type: 'AWS::SecretsManager::Secret'
        Properties:
          Description: 'My example secret'
          GenerateSecretString:
            SecretStringTemplate: '{""username"": ""${Username}"", ""password"": ""${Password}""}'
            GenerateStringKey: 'password'
            PasswordLength: 16
            ExcludePunctuation: true
          Tags:
            - Key: 'Name'
              Value: 'MySecret'
    Parameters:
      Username:
        Type: String
        Description: 'Username for the secret'
      Password:
        Type: String
        Description: 'Password for the secret'
    

&#x200B;",1708678764.0,1
swiggy,https://www.reddit.com/r/aws/comments/1axpid4/cloud_role/,Cloud Role,"Hey folks, I'm curious about how my standing compares to other upcoming fall 2024 graduates. My ultimate goal is to secure a position as a cloud engineer (AWS) right out of college. I understand it may seem ambitious, but I believe in aiming high. To date, I've completed four internships: one year as an IT intern, one year as a part-time cyber analyst, three months as a GRC intern, and my current role as a cyber range automation intern.

Throughout these experiences, I've successfully executed numerous projects and I've never stopped labing in my spare time. Additionally, I've earned my Sec+, Cloud+, and Secure Cloud Professional certs. Presently, I'm preparing for my AWS Cloud Architect certification, which I aim to achieve within the next three months. While I once held a CCNA, I'm contemplating on if I should renew it. Nevertheless, I retain the knowledge and skills gained from it, which is what I'm happy about.

I'm eager to hear your thoughts on how I stack up for a cloud engineer role. Furthermore, if you have any recommendations for additional resources that could enhance my preparation, I would greatly appreciate it. Thank you for taking the time to read my post.",1708656073.0,4
swiggy,https://www.reddit.com/r/aws/comments/1axkjgd/whats_your_experience_using_opensearch_anybody/,What's your experience using opensearch? Anybody ingesting lots of data?,"My use case is IoT ingesting maybe 100,000s of messages a minute. How does opensearch serverless scale. I've been stung by elastic before, has aws fixed anything?",1708642708.0,6
swiggy,https://www.reddit.com/r/aws/comments/1ax9fzh/comprehensive_guide_costperformance_compared_of/,Comprehensive Guide: Cost/Performance Compared of AWS File Storage Options,"Hello everyone!
>>>>>>> main

This code is from samples provided by AWS

Because other functions of the SDK aren't available for react-native.

<<<<<<< HEAD
    function createClientConfig(args: any): mqtt5.Mqtt5ClientConfig {
      let builder: iot.AwsIotMqtt5ClientConfigBuilder | undefined = undefined;
=======
2. Explanations of different storage and throughput modes

3. Insights into when to choose EFS over other AWS storage options  
4. Delves into EFS Elastic vs Bursting vs Provisioned, each type of EBS and FSx Lustre  
5. Price vs. Performance

Let me know your thoughts about it!",1708616549.0,16
swiggy,https://www.reddit.com/r/aws/comments/1axsycr/elastic_cache_redis_bug/,Elastic cache redis bug,"I had setup a production redis instance with the **Transit encryption** mode set as **User group access control list.** Everything was working smoothly since the deployment. We were able to connect to the instance through Lambda and the application was working as expected. Approximately 5 hours back, the **Transit encryption** mode got automatically changed to **Redis AUTH default user access.** Since then the application was unable to connect to the instance and our services were down. Is this normal? Has anyone else faced this?  
What should I do to prevent it from happening in the future?

P.s - I don't have a support plan for technical support from AWS.

Thanks!",1708666927.0,1
swiggy,https://www.reddit.com/r/aws/comments/1axscnv/help_with_running_pytorch_on_aws_lambda/,Help with Running PyTorch on AWS lambda,"Iím trying to run an AWS lambda function that uses PyTorch and Iím running into an error where NNPACK, a library that PyTorch uses, says itís out of memory even though there are 3gb of RAM left when memory usage is at its peak. Iím not sure what to do in this case. If this was vague I can provide more info. ",1708664919.0,1
swiggy,https://watch-aws-lambda-scale.com/,Watch AWS Lambda scale,,1708609148.0,16
swiggy,https://www.reddit.com/r/aws/comments/1axrbyz/is_aws_nfw_enterprise_grade/,"Is AWS NFW ""Enterprise Grade""?","We're using NFW for a landing zone, in central networking account, for all AWS traffic.

I was told recently by a colleague, that they normally see larger orgs using e.g. a Palo Virtual Appliance instead. And Platform colleagues I've spoken too have said they don't consider NFW to be Enterprise Grade.

For background - we made the decision wo use just NFW with input from some of our Platform crew, and our AWS Architect. Netsec (who manage the onprem Palo) didn't seem fussed one way or the other, so long as we did TLS inspection (for web, we're forwarding through a proxy that does it) (this was before NFW introduced TLS inspection on egress).

It's working pretty well and seems secure enough. We're using mainly the AWS-managed rule groups, plus domain filtering and some custom suricata rules, haven't hit any big problems.

In the past I've worked with onprem Palo and it was ok. I do note since NFW doesn't have anything like Wildfire with constantly updated rules based on emerging threats, that's a possible gap there.

I do also know with a Palo virtual appliance it'd hook into Panorama for centralized config & monitoring.

My question is, what other areas is NFW lacking in comparison to e.g. a Palo Virtual Appliance?",1708661596.0,1
swiggy,https://www.reddit.com/r/aws/comments/1axg8xb/alb_502_bad_gateway/,ALB 502 Bad Gateway,"Hi All,

I have an ECS service running a .NET 8 API. The container has port 8080 open. I am setting up an application load balancer to point to the ECS service using https:443. I am using a rule on the listener utilizing a subdomain. When I try hitting it, I get a 502 Bad Gateway. This only occurs on HTTPS; everything works fine on HTTP:80.

So, hereís all the details.

I have a healthcheck endpoint mapped in my API at /healthcheck

I have my ECS service running in a VPC with subnets us-east-1a and us-east-1b. This is running on Fargate.

I have my ALB in the same VPC and subnets. The ALB has an HTTPS listener on port 443. I have a rule on the listener that if the HTTP Host Header matches my subdomain, then it should forward to a target group.

The target group has a registered target with the IP address of my ECS service and a port of 8080. The target group is reporting the target is Healthy.

I have a security group on the ALB that accepts inbound on HTTP:80 and HTTPS:443.

I have a security group on the ECS service that accepts inbound on port 8080.

I have a wildcard certificate from ACM on the HTTPS listener that fits my subdomain.

Under the monitoring of my ALB, I see spikes in these categories: ELB 5XXs, HTTP 502s, Target TLS Negotiation Errors, Client TLS Negotiation Errors.

Are any of those indications of the ALB or my ECS service is the issue?

If I setup all my same rules and everything but using the HTTP listener minus the ACM certificate, all works well.

I feel Iíve hit a wall in trying to figure this out so any insight is much appreciated.",1708632518.0,4
swiggy,https://www.reddit.com/r/aws/comments/1axqa0f/aws_findmatches_confidence_scores_null_confidence/,AWS FindMatches Confidence Scores (Null confidence for subset of matches),"I am trying to use the FindMatches transformation, and have already gone through several rounds of training, tuning, etc.  My training set has gone through a few iterations and includes only hand-labeled data, with edge cases and normal cases all included.  The transformation is doing really well for most of the records.

The big question I have is that I am experiencing a large number of rows coming back with no match score at all.  I can't seem to see any mention of this online.  It is not 0 confidence, it is just no number at all in the output.  I cannot see any pattern to the records that are getting this behavior.  

Anyone know anything about this?",1708658334.0,1
swiggy,https://www.reddit.com/r/aws/comments/1axl177/why_do_we_still_need_to_scale_with_aws_shield_and/,Why do we still need to scale with AWS Shield and WAF caught all the attack?,Might be a noob question but I dont understand why every time we talk to AWS SRT they always recommended to still need to autoscale the resources behind WAF where AWS Shield and WAF block all of the DDoS attack? Why would the volume of traffic still reach our endpoint when it got clipped by WAF and auto ddos mitigation integration,1708643885.0,2
swiggy,https://www.reddit.com/r/aws/comments/1axjnmf/do_i_need_to_care_about_vpc_security_groups_waf/,"Do I need to care about VPC, security groups, WAF, etc.?","I have the following setup:

* Frontend: Route53 -> Cloudfront > S3 (React app).
* Backend: API Gateway -> Lambda Function (Proxy Integration) -> MongoDB Atlas.
* Blob Storage: S3 buckets to store files.

I was watching this video today of an AWS security specialist and listened to him talk about all of these terms VPC, Availability Zone, security groups, WAF, etc.

Is this something I need to worry about with my current setup or is this more geared towards big organisations? Also, he only mentioned EC2 for this setup which got me wondering if all of these concepts make sense for a serverless approach with api gateway and lambda like I'm using.",1708640585.0,2
swiggy,https://www.reddit.com/r/aws/comments/1axf4je/aws_alternatives_to_replace_airflow_for_data/,AWS alternatives to replace Airflow for data pipelines,"The current set up on the company I'm working uses Airflow DAGs to:

Gather data from a source > run some scripts (on Databricks notebooks) using the data > upload the data on a database if everything goes right. The scripts also use some variables set on airflow and DynamoDB.

The directors want an alternative to Airflow because the cost to keep it is too high, considering these pipelines run once per hour, so I've been searching for a solution. 

&#x200B;

Currently what I'm thinking of is using Step Functions to set workflows similar to the DAGs on Airflow, combined with Lambda functions that will run the databricks scripts. I have some doubts about this set up such as if I will also need EventBridge to set the schedulers for the workflows, or is there a way to parametrize the schedulers directly on the Step functions workflow?

&#x200B;

If anyone got some tips about my idea or suggestions for other services that can accomplish what I need, I'd be happy to hear about it.",1708629884.0,3
swiggy,https://www.reddit.com/r/aws/comments/1ax9hwq/captcha_loop_on_console_signin_am_i_going_insane/,Captcha loop on Console sign-in. Am I going insane?,"I'm in my first Captcha loop (I learned this term today) and my mind is blown by how useless AWS are in resolving it. 

I've gotten a number of boilerplate, refer-to-this-article fob-offs from the Support team, however I'm staggered that this is even a thing. 

Feels like some automated process somewhere has decided I'm a bot and nobody at AWS has any kind of admin power over this. 

Utterly shocked. ",1708616682.0,5
swiggy,https://www.reddit.com/r/aws/comments/1axae6e/do_i_need_to_limit_the_amount_of_records_returned/,Do I need to limit the amount of records returned from a DDB query?,"I'm currently following the DDB docs for pagination with looping through until last evaluated key is no longer there. 

Do i also need to consider limiting the amount of results returned from my query function? There doesn't seem to be a simple way of doing so. 

The ""Limit"" parameter only allows me to set the amount of records returned per page. 

Just thinking of how to prevent a ""runaway"" query.. per our architect.. but i suggested we just harden how we are calling our queries and what values/filters are passed.. create GSIs if necessary etc... but he's not having it lol

Am I wrong?

This is for an API that will call DDB and return some records that are enriched to the consumer. The nature of the return payload doesn't really benefit the user with a ""max results"" returned either

What would you suggest?

Also.. i'm using API GW > Lambda (python/boto3)",1708618817.0,3
swiggy,https://www.reddit.com/r/aws/comments/1axj8gz/bug_in_lambda_function_attempts_to_terminate_all/,Bug in lambda function attempts to terminate all EC2s instead of just EC2s scoped to a specific ASG,"I have a solution involving an ASG spawning stateful workers which poll messages from an SQS queue, process data, and self-terminate. Because I don't want a scale in event to terminate a worker mid-process, I enable `scale-in protection` on the workers, and have workers explicitly self-terminate when finished processing. However, I want to have an external way to ensure these workers are terminated so that an uncaught error prevents the self-terminating logic from running (the ASG `Maximum instance lifetime` parameter doesn't seem to be able to terminate instances with `scale-in protection`).

I decided to do this simply and schedule a lambda to identify and terminate all long-running workers (>8 hours) in the ASG, and it looked like everything was working fine! ASG instances were terminated...

... However, I came back the next day and realized that this function also attempts to terminate instances outside of the ASG. I've looked through the logic but I can't see why the terminated instances aren't correctly scoped to the ASG?

    import json
    import boto3
    from datetime import datetime, timedelta
>>>>>>> main
    
      let wsOptions: iot.WebsocketSigv4Config | undefined = undefined;
      if (args.region) {
        wsOptions = {
          region: args.region,
          // credentialsProvider: auth.AwsCredentialsProvider.newDefault(),
        };
      }
    
      builder =
        iot.AwsIotMqtt5ClientConfigBuilder.newWebsocketMqttBuilderWithSigv4Auth(
          args.endpoint,
          wsOptions
        );
      builder.withCertificateAuthorityFromPath(undefined, args.cert);
    
      builder.withConnectProperties({
        keepAliveIntervalSeconds: 1200,
      });
    
      return builder.build();
    }function createClientConfig(args: any): mqtt5.Mqtt5ClientConfig {
      let builder: iot.AwsIotMqtt5ClientConfigBuilder | undefined = undefined;
    
    
      let wsOptions: iot.WebsocketSigv4Config | undefined = undefined;
      if (args.region) {
        wsOptions = {
          region: args.region,
          // credentialsProvider: auth.AwsCredentialsProvider.newDefault(),
        };
      }
    
    
      builder =
        iot.AwsIotMqtt5ClientConfigBuilder.newWebsocketMqttBuilderWithSigv4Auth(
          args.endpoint,
          wsOptions
        );
      builder.withCertificateAuthorityFromPath(undefined, args.cert);
    
    
      builder.withConnectProperties({
        keepAliveIntervalSeconds: 1200,
      });
    
    
      return builder.build();
    }



`credentialsProvider: auth.AwsCredentialsProvider.newDefault()`

This line was suggested by GPT which didn't do shit.

I guess I need to add some sort of auth on the AWS side first and then use that to access the endpoint from here. But what and how?

The method below works fine when run in Node, but it's not available on react-native

    if (args.key && args.cert) {
     builder =
       iot.AwsIotMqtt5ClientConfigBuilder.newDirectMqttBuilderWithMtlsFromPath(
         args.endpoint,
         args.cert,
         args.key
       );

I've tried several things to resolve it, but I'm still scratching my head.

There's a few tutorials about slightly different things that talk about doing something with Cognito or Amplify but I couldn't really understand them (or didn't want to). I thought that maybe there was a simpler way

This is my first time dabbling in IoT, AWS and React Native.

I truly appreciate any insights or suggestions you can offer!",1708837081.0,1
aws,https://www.reddit.com/gallery/1azcz6i,Cloud Quest - API Gateway,"Can someone please help me with this one. I am now too invested in this game and I cant just ignore it. Hahaha ü•π

And I got this result though I did everything. I already spent 1hour configuring it but still the result was ""AN ERROR (403) OCCURED WHILE INVOKING API WITH THE URL YOU PROVIDED.""",1708828822.0,0
aws,https://www.reddit.com/r/aws/comments/1aza0fo/java_shared_code_in_a_lambda_layer_how_will/,[Java] Shared code in a lambda layer. How will lambdas access this layer?,"I have 5+ lambdas. Each lambda does its work by using 1 specific class and multiple utility classes. All the lambdas live in the same repository. Their `build.gradle`s all have the same dependencies. Small example:

    CreateLambda
      src
       main
        CreateHandler.java
        CommonUtility1.java
        CommonUtility2.java
      build.gradle

    UpdateLambda
      src
       main
        UpdateHandler.java
        CommonUtility1.java
        CommonUtility2.java
      build.gradle

This results in identical utility classes being duplicated everywhere. Therefore I am looking into creating a `lambda layer` that will contain all the common dependencies and shared utility classes that the 3 lambdas will use. Small updated example:

    LambdaLayer
      src
       main
        CommonUtility1.java
        CommonUtility2.java
      build.gradle

    CreateLambda
      src
       main
        CreateHandler.java
      build.gradle

    UpdateLambda
      src
       main
        UpdateHandler.java
      build.gradle

My question is: **How would the lambdas import the lambda layer as a dependency?** Would they just have to import it as a dependency in the `build.gradle` as they would with any other dependency? That would mean that the lambda layer JAR would have to be published first before the lambdas can use it right? This is for work and at my company all JARs get published to a company maven repository.

Anything else I need to know in order to do this properly? Lambda layer tutorials in Java are rare when I google",1708820419.0,1
aws,https://www.reddit.com/r/aws/comments/1az9s0s/video_on_demand_via_aws_media_convert/,Video on Demand via AWS Media Convert?,"Hello, using S3 and cloudfront for video on demand services(50mb - 5gb files), however data transfer costs are getting out of hand;

1. Is media convert a suitable approach(cost, overhead, efficacy, etc)?
2. Does media convert have adaptive bitrate streaming?
3. Id like to load videos in ""chunks"" instead of loading entire file like i am currently, does media convert help with this?",1708819813.0,1
aws,https://www.reddit.com/r/aws/comments/1az94ga/video_on_demand_options/,Video On Demand options?,"Currently storing via S3 and serving via CloudFront, however the data transfer costs are still too high and id like to find a solution:

1. Should I even be using s3 + CloudFront for this?
2. What is adaptive bitrate? What are my options for this?
3. Should I convert/compress videos upon upload? If so what are my options
4. Proper CloudFront utlization? Currently im signing cloudfront URLs and sending it directly to client, does cloudfront automatically cache and compress files or must I do something else?",1708818106.0,0
aws,https://www.reddit.com/r/aws/comments/1az8hwb/is_it_possible_to_grant_a_lightsail_instance/,Is it possible to grant a Lightsail instance access to Bedrock via Service Roles?,"I have a Lighsail instance that I would like to make requests to my Bedrock FM. I could create Access Key/Access Key Secret, but I am wondering if this is possible via Service Roles. It seems Lightsail uses a Service Linked Role, which cannot be modified. I just want to make sure I am understanding that correctly.

[https://docs.aws.amazon.com/IAM/latest/UserGuide/reference\_aws-services-that-work-with-iam.html#all\_svcs](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html#all_svcs)",1708816473.0,1
aws,https://www.reddit.com/r/aws/comments/1ayy7hq/aws_cognito_giving_me_code_that_violates_their/,AWS Cognito giving me code that violates their own policy?,"I am creating a website using AWS S3 and I am using Cognito for user accounts.  


I have Cognito set up and working on my website to where a user can sign up for an account. However, whenever a user is directed to the link provided by Cognito for authentication (ex: https://<website name>-login.auth.us-east-1.amazoncognito.com/login?client\_id=<client id>&response\_type=code&scope=email+openid&redirect\_uri=https%3A%2F%2F<domain name>%2F<website\_name>.html), the following error is had in the console:  


""Refused to execute inline script because it violates the following Content Security Policy directive: ""script-src [https://d3oia8etllorh5.cloudfront.net](https://d3oia8etllorh5.cloudfront.net) https://<website name>-login.auth.us-east-1.amazoncognito.com"". Either the 'unsafe-inline' keyword, a hash ('sha256-fWEffNYGDN+XQ+YrsU/LKDZAnOcnSlra1fOIm+7oskM='), or a nonce ('nonce-...') is required to enable inline execution.""  


Basically, it is saying that on this authentication website provided by Cognito, it uses an inline script, which is not allowed by AWS' Content Security Policy. When opening up the code for the authentication website via ""Inspect"", there is in fact inline script.  


How come AWS gave me code that violates their own CSP Policy? How do I get around this? I've tried many things, but nothing seems to work.  


Thanks for any help!",1708791127.0,4
aws,https://www.reddit.com/r/aws/comments/1aym3nw/how_do_you_implement_platform_engineering/,How do you implement platform engineering??,"Okay, I‚Äôm working as a sr ‚Äúdevops‚Äù engineer with a software developer background trying to build a platform for a client. I‚Äôll try to keep my opinions out of it, but I don‚Äôt love platform engineering and I don‚Äôt understand how it could possibly scale‚Ä¶at least not with what we have built. 

Some context, we are using a gitops approach for deploying infrastructure onto aws. We use Kubernetes based terraform operator (yeah questionable‚Ä¶I know) and ArgoCD to manage deployments of infra. 

We created several terraform modules that contain a SINGLE aws resource in its own git repository. There are some ‚Äúsensible defaults‚Äù in the modules and a bunch of variables for users to input if they choose or not. Tons of conditional logic in the templates. 

Our plan is to enable these to be consumed through an IDP (internal developer portal) to give devs an easy button. 

My question is, how does this scale. It‚Äôs very challenging to write single modules that can be deployed with their own individual terraform state. So I can‚Äôt reference outputs and bind resources together very easily without multi step deployments sometimes. Or guessing at what the output name of a resource might be. 

For example, it‚Äôs very hard to do this with a native aws cloud solution like s3 bucket that triggers lambda based on putObject that then sends a message to sqs and is consumed by another lambda. Or triggering a lambda based on RDS input etc etc. 

So, my question is how do you make a ‚Äúplatform/product‚Äù that allows for flexibility for product teams and devs to consume services through a UI or some easy button without writing the terraform themselves?? 

TL;DR: 
How do you write terraform modules in a platform? ",1708750638.0,20
aws,https://www.reddit.com/r/aws/comments/1az3tm4/nvidia_driver_for_g5xlarge/,Nvidia driver for g5.xlarge,"Hello,

I just take a g5.xlarge  with an Amazon Linux 2023 AMI and I struggle to install an nvidia driver,  so please I have some questions:

How can I install an nvida driver there ? I konw there is some AMI which comes already with a driver but I don't want thid solution

Which driver should I use ? My purpose is to play with llama2 to do some GenAI (RAG)

Are nvidia drivers free ?

Thanks a lot
",1708804929.0,0
aws,https://www.reddit.com/r/aws/comments/1az31m3/aws_cli_how_can_i_filter_a_file_list_from_the_ls/,[AWS CLI] how can i filter a file list from the ls command?,"Is there a way to filter a file listing from the ls command in the AWS CLI? e.g. What would I have to do (from the command line) if I just want to see or download a list of all the zip files in a folder that has over a million files where only some are zips? I tried:  
  
aws s3 ls s3://(bucket)/(folder) --exclude ""*"" --include ""*.zip""  
  
but that doesn't work - surely there must be some way to do this? I guess it'd take the same amount of time to list the results either way but it'd be nice to be able to get a filtered result (and a smaller text output file) after the wait rather than having to do manually filter that for what i want afterwards.",1708802953.0,1
aws,https://www.reddit.com/r/aws/comments/1ayxgg5/questions_on_org_trail_in_control_tower/,Question(s) on Org Trail in Control Tower,"Hello,

I would appreciate if some kind soul could give me pointers on what I am trying to achieve. I may not be using the correct search terms when looking around the interwebs.

 We are getting started with our AWS journey with Control Tower being used to come up with a well architected framework as recommended by AWS.

The one thing I am a bit confused about is, how do we monitor all the CloudTrail events in the ""Audit"" account with our own custom alert.  The Control Tower framework has created the OrgTrail with the Audit account having access to all accounts events, I see AWS Guard Duty monitoring and occasionally alerting me on stuff.  

Q1: How do I extend the alerting above and beyond what AWS Guard Duty does?  

Q2: We are comfortable with our on-prem SIEM and although I am aware of the costs involved in bringing in CloudTrail events through our OrgTrail, it is something we are comfortable with to get started.  How do I do this?  I am assuming this is possible.  

Thank you all!

GT

&#x200B;",1708789240.0,2
aws,https://www.reddit.com/r/aws/comments/1az2jsb/deployment/,Deployment,"I need yalls help!! 

I've never used aws before and i have a question regarding the deployment of a website. 

Im working on a school project and the topic is gitlab ci/cd pipeline. I've developed my own web app using flask and angular and i have two separate folders for each of those. The app is for demonstrational purposes only, nobody would actually be using it. How do i deploy that? Ive read that for backend i could probably use beanstalk? What about the frontend? Do you make two separate jobs in a pipeline for backend and frontend? How do they communicate in the cloud? 

Lol, i have so many questions and no idea where to start. Any courses/literature recommendations are welcomed. ",1708801726.0,0
aws,https://www.reddit.com/r/aws/comments/1ayr108/combining_aurora_serverless_v2_with_reserved/,Combining Aurora Serverless v2 with Reserved Instances,"Hey everyone, I'd like to validate an approach before diving a rabbit hole of setup:

* We have a fairly standard baseline use to our Aurora databases, so we use a couple r7g.large instances in each of the major regions we operate. This is great 99.9% of the time.
* However, within each region things can get spikey - often only for a few minutes each day - which can cause degraded performance as we go over our provisioned vCPUs.
* My reading is that you can setup Aurora Serverless v2 to sit as a reader *alongside* regular RI readers, so traffic can go to both types at the same time, and this will be handled by RDS Proxy.

So the core assumption I'm looking to validate: **can I use RIs to cut costs for our baseline usage and use Aurora Serverless v2 to sit on standby to handle spikiness?** I realise there might be an obvious answer based on my last bullet point above, but every blog post I'm reading always compares the two services as an either/or option, and typically concludes that Aurora Serverless v2 ends up simply being more expensive because it doesn't make sense for baseline usage. I haven't found any posts of people talking about this setup.

Follow on questions that I'm also not getting clear answers on:

* Is RDS Proxy intelligent enough when you're using readers and serverless together (i.e. will it start sending spiky traffic to serverless, rather than splitting amongst all readers?)
* Can our apu be left at 0.5 and it will autoscale to any capacity we need, or should we try and optimise our apu minimum to be equal to the memory our database typically needs? Is there a simple way of working this out?
* What are the other gotchas that we should be considering?

Final more unrelated question:

* We don't have an AWS support contract today. If we had one, would they be good at answering these types of questions, or are they more useful for just having someone to call when something goes wrong?",1708768936.0,6
aws,https://www.reddit.com/r/aws/comments/1ayplbk/lambda_function_authentication/,Lambda function authentication ,"Really new to all this stuff. I have a lambda function talking to OpenAI api which accessible via an endpoint (API gateway). This endpoint is being called from my react native app. 

The whole reason to create this function was because I did not want to store the api key in the app code. 

Now, I am facing issue with authenticating this endpoint. What simple yet secure enough solutions can I use to authenticate my endpoint? Another api key might be a solution but again it gets exposed client side",1708763157.0,6
aws,https://www.reddit.com/r/aws/comments/1aytl6f/lambdaedge_cold_start/,Lambda@Edge cold start?,"Hello

Is there any way to keep lambda @ edge warmed up? 

Otherwise, it ends up slowing down the website significantly... which is counter intuitive ",1708778364.0,3
aws,https://www.reddit.com/r/aws/comments/1aywm89/any_python_libraries_out_there_that_mock_the/,Any Python libraries out there that mock the MultiProcessing module and instead pass code to AWS Lambda to run?,"There are isolated functions / modules in my code that get invoked \~100k times (around 3 minutes for each invocation) each time I launch my code.  I can easily make this parallel using Python's multiprocessing module, but even with multiprocessing it'll take forever to run. Is there an interface similar to multiprocessing that pipes to lambda instead? The isolated functions are in heavy development, so I'm trying to avoid having to futz with aws-cli, AWS Console, IAM permissions, downloading from S3, etc... everytime I make a small change. I just want to run and have the results come back directly into Python.

Does such a thing exist?",1708787084.0,1
aws,https://www.reddit.com/r/aws/comments/1aywfuv/aws_policy_need_explaination_for_the_forallvalues/,"AWS policy: need explaination for the ""ForAllValues"" qualifier","I have this SCP attached to account A in my org

    {
      ""Version"": ""2012-10-17"",
      ""Statement"": [
        {
          ""Effect"": ""Deny"",
          ""Action"": [
            ""route53:ChangeResourceRecordSets""
          ],
          ""Resource"": ""*"",
          ""Condition"": {
            ""ForAllValues:StringEquals"": {
              ""route53:ChangeResourceRecordSetsRecordTypes"": [
                ""A"",
                ""AAAA""
              ]
            }
          }
        }
      ]
    }

<<<<<<< HEAD
As I understand:

* the SCP will deny if each and every record's type in my create-record request equals A or AAAA
* the SCP will not deny if at least one record's type in my create-record request is not equal A or AAAA

However, when I create R53 records in account A using the console:

https://preview.redd.it/kn6lkirqrjkc1.png?width=3360&format=png&auto=webp&s=e2de7f40ae3affcb39578ce271047ab4051bedf3

the SCP still blocks the request even though the request contains a record of type ""CNAME"" which is not A or AAAA

What is wrong in my understanding of this ""ForAllValues"" ? Please help me. Thanks",1708786626.0,1
aws,https://www.reddit.com/r/aws/comments/1ayrqmq/is_it_recommended_to_use_aws_to_host_docker/,is it recommended to use aws to host docker containers,"so I have a docker image that I have pushed to the docker hub registry. Now I have to host it somewhere and I was looking at some of the different options. AWS seems really good but i keep reading that they have a free tier but its not actually free and people being charged thousands of dollars unknowingly. If I were to start playing around with AWS today, how likely would it be that something like that would happen to me? Are there any other options I should try instead? thanks",1708771695.0,2
aws,https://www.reddit.com/r/aws/comments/1ay99so/bedrock_adds_mistral_ai_models/,Bedrock adds Mistral AI models,"even more AI models are being added to Bedrock:  


[https://www.aboutamazon.com/news/aws/mistral-ai-amazon-bedrock](https://www.aboutamazon.com/news/aws/mistral-ai-amazon-bedrock)",1708716383.0,21
aws,https://www.reddit.com/r/aws/comments/1ay9pon/is_anyone_using_codecommit_codebuild_codedeploy/,"Is anyone using CodeCommit, CodeBuild, CodeDeploy and CodePipeline?","I am currently studying for the AWS DevOps Professional certificate. On A Cloud Guru there is this section about CodeCommit, CodeBuild, CodeDeploy and CodePipeline. Why would I use any of those services if I can also use Github and Github Actions?",1708717463.0,18
aws,https://www.reddit.com/r/aws/comments/1ayykyl/aws_transcribe_is_crazy_expensive_how_to_lower/,AWS Transcribe is crazy expensive. How to lower costs and keep comparable performance?,"Looking for information on AWS hosted alternatives, e.g. Whisper on EC2. Has anyone switched over to this? If so how low are you able to get your costs to transcribe audio and video with comparable quality?",1708792054.0,16
aws,https://www.reddit.com/r/aws/comments/1azggur/one_vs_more_vpn/,One vs more VPN,"We are debating if se should use one or more Customer Gateway/VPN connections with our Transit Gateway. To me it looks like it just increases how much resources we need to maintain(more in Aws and onprem both), without adding much resiliency.

As I can tell our current CGW/connection have never been down, did AWS ever have outtage id this service?
And if it did, it would impact us the same no matter if we had 1 or 4 CGWs as they would all be affected.",1708840108.0,1
aws,https://www.reddit.com/r/aws/comments/1azburu/new_accounts_under_organisation_are_not/,New accounts under organisation are not consolidating billing,"I have a personal AWS org. The management account is setup with billing and so I can create resources within it. Obviously, I don't want to use the management account for that - but every time I create a new account in the org it is considered ""NotSignedUp"", so accessing S3 will 404, etc. 

As I understand it, any new/invited account within an org now, by default, consolidates its billing under the management account?

Does anyone know why new accounts might be treated as NotSignedUp?",1708825514.0,2
aws,https://www.reddit.com/r/aws/comments/1azfkay/mqtt_connection_failure_with_aws_iot_core/,MQTT connection failure with AWS IoT Core,"I need some assistance with a frustrating issue I'm encountering while connecting my device to AWS IoT Core. I'm using the \`aws-iot-device-sdk-v2\` library for JS and keep hitting this error:

`Error: aws-c-io: AWS_IO_TLS_ERROR_NEGOTIATION_FAILURE, TLS (SSL) negotiation failed`

This happens when I try to connect using `newWebsocketMqttBuilderWithSigv4Auth`

This code is from samples provided by AWS

Because other functions of the SDK aren't available for react-native.

    function createClientConfig(args: any): mqtt5.Mqtt5ClientConfig {
      let builder: iot.AwsIotMqtt5ClientConfigBuilder | undefined = undefined;
    
      let wsOptions: iot.WebsocketSigv4Config | undefined = undefined;
      if (args.region) {
        wsOptions = {
          region: args.region,
          // credentialsProvider: auth.AwsCredentialsProvider.newDefault(),
        };
      }
    
      builder =
        iot.AwsIotMqtt5ClientConfigBuilder.newWebsocketMqttBuilderWithSigv4Auth(
          args.endpoint,
          wsOptions
        );
      builder.withCertificateAuthorityFromPath(undefined, args.cert);
    
      builder.withConnectProperties({
        keepAliveIntervalSeconds: 1200,
      });
    
      return builder.build();
    }function createClientConfig(args: any): mqtt5.Mqtt5ClientConfig {
      let builder: iot.AwsIotMqtt5ClientConfigBuilder | undefined = undefined;
    
    
      let wsOptions: iot.WebsocketSigv4Config | undefined = undefined;
      if (args.region) {
        wsOptions = {
          region: args.region,
          // credentialsProvider: auth.AwsCredentialsProvider.newDefault(),
        };
      }
    
    
      builder =
        iot.AwsIotMqtt5ClientConfigBuilder.newWebsocketMqttBuilderWithSigv4Auth(
          args.endpoint,
          wsOptions
        );
      builder.withCertificateAuthorityFromPath(undefined, args.cert);
    
    
      builder.withConnectProperties({
        keepAliveIntervalSeconds: 1200,
      });
    
    
      return builder.build();
    }



`credentialsProvider: auth.AwsCredentialsProvider.newDefault()`

This line was suggested by GPT which didn't do shit.

I guess I need to add some sort of auth on the AWS side first and then use that to access the endpoint from here. But what and how?

The method below works fine when run in Node, but it's not available on react-native

    if (args.key && args.cert) {
     builder =
       iot.AwsIotMqtt5ClientConfigBuilder.newDirectMqttBuilderWithMtlsFromPath(
         args.endpoint,
         args.cert,
         args.key
       );

I've tried several things to resolve it, but I'm still scratching my head.

There's a few tutorials about slightly different things that talk about doing something with Cognito or Amplify but I couldn't really understand them (or didn't want to). I thought that maybe there was a simpler way

This is my first time dabbling in IoT, AWS and React Native.

I truly appreciate any insights or suggestions you can offer!",1708837081.0,1
aws,https://www.reddit.com/gallery/1azcz6i,Cloud Quest - API Gateway,"Can someone please help me with this one. I am now too invested in this game and I cant just ignore it. Hahaha ü•π

And I got this result though I did everything. I already spent 1hour configuring it but still the result was ""AN ERROR (403) OCCURED WHILE INVOKING API WITH THE URL YOU PROVIDED.""",1708828822.0,0
aws,https://www.reddit.com/r/aws/comments/1aza0fo/java_shared_code_in_a_lambda_layer_how_will/,[Java] Shared code in a lambda layer. How will lambdas access this layer?,"I have 5+ lambdas. Each lambda does its work by using 1 specific class and multiple utility classes. All the lambdas live in the same repository. Their `build.gradle`s all have the same dependencies. Small example:

    CreateLambda
      src
       main
        CreateHandler.java
        CommonUtility1.java
        CommonUtility2.java
      build.gradle

    UpdateLambda
      src
       main
        UpdateHandler.java
        CommonUtility1.java
        CommonUtility2.java
      build.gradle

This results in identical utility classes being duplicated everywhere. Therefore I am looking into creating a `lambda layer` that will contain all the common dependencies and shared utility classes that the 3 lambdas will use. Small updated example:

    LambdaLayer
      src
       main
        CommonUtility1.java
        CommonUtility2.java
      build.gradle

    CreateLambda
      src
       main
        CreateHandler.java
      build.gradle

    UpdateLambda
      src
       main
        UpdateHandler.java
      build.gradle

My question is: **How would the lambdas import the lambda layer as a dependency?** Would they just have to import it as a dependency in the `build.gradle` as they would with any other dependency? That would mean that the lambda layer JAR would have to be published first before the lambdas can use it right? This is for work and at my company all JARs get published to a company maven repository.

Anything else I need to know in order to do this properly? Lambda layer tutorials in Java are rare when I google",1708820419.0,1
aws,https://www.reddit.com/r/aws/comments/1az9s0s/video_on_demand_via_aws_media_convert/,Video on Demand via AWS Media Convert?,"Hello, using S3 and cloudfront for video on demand services(50mb - 5gb files), however data transfer costs are getting out of hand;

1. Is media convert a suitable approach(cost, overhead, efficacy, etc)?
2. Does media convert have adaptive bitrate streaming?
3. Id like to load videos in ""chunks"" instead of loading entire file like i am currently, does media convert help with this?",1708819813.0,1
aws,https://www.reddit.com/r/aws/comments/1az94ga/video_on_demand_options/,Video On Demand options?,"Currently storing via S3 and serving via CloudFront, however the data transfer costs are still too high and id like to find a solution:

1. Should I even be using s3 + CloudFront for this?
2. What is adaptive bitrate? What are my options for this?
3. Should I convert/compress videos upon upload? If so what are my options
4. Proper CloudFront utlization? Currently im signing cloudfront URLs and sending it directly to client, does cloudfront automatically cache and compress files or must I do something else?",1708818106.0,0
aws,https://www.reddit.com/r/aws/comments/1az8hwb/is_it_possible_to_grant_a_lightsail_instance/,Is it possible to grant a Lightsail instance access to Bedrock via Service Roles?,"I have a Lighsail instance that I would like to make requests to my Bedrock FM. I could create Access Key/Access Key Secret, but I am wondering if this is possible via Service Roles. It seems Lightsail uses a Service Linked Role, which cannot be modified. I just want to make sure I am understanding that correctly.

[https://docs.aws.amazon.com/IAM/latest/UserGuide/reference\_aws-services-that-work-with-iam.html#all\_svcs](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html#all_svcs)",1708816473.0,1
aws,https://www.reddit.com/r/aws/comments/1ayy7hq/aws_cognito_giving_me_code_that_violates_their/,AWS Cognito giving me code that violates their own policy?,"I am creating a website using AWS S3 and I am using Cognito for user accounts.  


I have Cognito set up and working on my website to where a user can sign up for an account. However, whenever a user is directed to the link provided by Cognito for authentication (ex: https://<website name>-login.auth.us-east-1.amazoncognito.com/login?client\_id=<client id>&response\_type=code&scope=email+openid&redirect\_uri=https%3A%2F%2F<domain name>%2F<website\_name>.html), the following error is had in the console:  


""Refused to execute inline script because it violates the following Content Security Policy directive: ""script-src [https://d3oia8etllorh5.cloudfront.net](https://d3oia8etllorh5.cloudfront.net) https://<website name>-login.auth.us-east-1.amazoncognito.com"". Either the 'unsafe-inline' keyword, a hash ('sha256-fWEffNYGDN+XQ+YrsU/LKDZAnOcnSlra1fOIm+7oskM='), or a nonce ('nonce-...') is required to enable inline execution.""  


Basically, it is saying that on this authentication website provided by Cognito, it uses an inline script, which is not allowed by AWS' Content Security Policy. When opening up the code for the authentication website via ""Inspect"", there is in fact inline script.  


How come AWS gave me code that violates their own CSP Policy? How do I get around this? I've tried many things, but nothing seems to work.  


Thanks for any help!",1708791127.0,2
aws,https://www.reddit.com/r/aws/comments/1aym3nw/how_do_you_implement_platform_engineering/,How do you implement platform engineering??,"Okay, I‚Äôm working as a sr ‚Äúdevops‚Äù engineer with a software developer background trying to build a platform for a client. I‚Äôll try to keep my opinions out of it, but I don‚Äôt love platform engineering and I don‚Äôt understand how it could possibly scale‚Ä¶at least not with what we have built. 

Some context, we are using a gitops approach for deploying infrastructure onto aws. We use Kubernetes based terraform operator (yeah questionable‚Ä¶I know) and ArgoCD to manage deployments of infra. 

We created several terraform modules that contain a SINGLE aws resource in its own git repository. There are some ‚Äúsensible defaults‚Äù in the modules and a bunch of variables for users to input if they choose or not. Tons of conditional logic in the templates. 

Our plan is to enable these to be consumed through an IDP (internal developer portal) to give devs an easy button. 

My question is, how does this scale. It‚Äôs very challenging to write single modules that can be deployed with their own individual terraform state. So I can‚Äôt reference outputs and bind resources together very easily without multi step deployments sometimes. Or guessing at what the output name of a resource might be. 

For example, it‚Äôs very hard to do this with a native aws cloud solution like s3 bucket that triggers lambda based on putObject that then sends a message to sqs and is consumed by another lambda. Or triggering a lambda based on RDS input etc etc. 

So, my question is how do you make a ‚Äúplatform/product‚Äù that allows for flexibility for product teams and devs to consume services through a UI or some easy button without writing the terraform themselves?? 

TL;DR: 
How do you write terraform modules in a platform? ",1708750638.0,19
aws,https://www.reddit.com/r/aws/comments/1az3tm4/nvidia_driver_for_g5xlarge/,Nvidia driver for g5.xlarge,"Hello,

I just take a g5.xlarge  with an Amazon Linux 2023 AMI and I struggle to install an nvidia driver,  so please I have some questions:

How can I install an nvida driver there ? I konw there is some AMI which comes already with a driver but I don't want thid solution

Which driver should I use ? My purpose is to play with llama2 to do some GenAI (RAG)

Are nvidia drivers free ?

Thanks a lot
",1708804929.0,0
aws,https://www.reddit.com/r/aws/comments/1az31m3/aws_cli_how_can_i_filter_a_file_list_from_the_ls/,[AWS CLI] how can i filter a file list from the ls command?,"Is there a way to filter a file listing from the ls command in the AWS CLI? e.g. What would I have to do (from the command line) if I just want to see or download a list of all the zip files in a folder that has over a million files where only some are zips? I tried:  
  
aws s3 ls s3://(bucket)/(folder) --exclude ""*"" --include ""*.zip""  
  
but that doesn't work - surely there must be some way to do this? I guess it'd take the same amount of time to list the results either way but it'd be nice to be able to get a filtered result (and a smaller text output file) after the wait rather than having to do manually filter that for what i want afterwards.",1708802953.0,1
aws,https://www.reddit.com/r/aws/comments/1ayxgg5/questions_on_org_trail_in_control_tower/,Question(s) on Org Trail in Control Tower,"Hello,

I would appreciate if some kind soul could give me pointers on what I am trying to achieve. I may not be using the correct search terms when looking around the interwebs.

 We are getting started with our AWS journey with Control Tower being used to come up with a well architected framework as recommended by AWS.

The one thing I am a bit confused about is, how do we monitor all the CloudTrail events in the ""Audit"" account with our own custom alert.  The Control Tower framework has created the OrgTrail with the Audit account having access to all accounts events, I see AWS Guard Duty monitoring and occasionally alerting me on stuff.  

Q1: How do I extend the alerting above and beyond what AWS Guard Duty does?  

Q2: We are comfortable with our on-prem SIEM and although I am aware of the costs involved in bringing in CloudTrail events through our OrgTrail, it is something we are comfortable with to get started.  How do I do this?  I am assuming this is possible.  

Thank you all!

GT

&#x200B;",1708789240.0,2
aws,https://www.reddit.com/r/aws/comments/1az2jsb/deployment/,Deployment,"I need yalls help!! 

I've never used aws before and i have a question regarding the deployment of a website. 

Im working on a school project and the topic is gitlab ci/cd pipeline. I've developed my own web app using flask and angular and i have two separate folders for each of those. The app is for demonstrational purposes only, nobody would actually be using it. How do i deploy that? Ive read that for backend i could probably use beanstalk? What about the frontend? Do you make two separate jobs in a pipeline for backend and frontend? How do they communicate in the cloud? 

Lol, i have so many questions and no idea where to start. Any courses/literature recommendations are welcomed. ",1708801726.0,0
aws,https://www.reddit.com/r/aws/comments/1ayr108/combining_aurora_serverless_v2_with_reserved/,Combining Aurora Serverless v2 with Reserved Instances,"Hey everyone, I'd like to validate an approach before diving a rabbit hole of setup:

* We have a fairly standard baseline use to our Aurora databases, so we use a couple r7g.large instances in each of the major regions we operate. This is great 99.9% of the time.
* However, within each region things can get spikey - often only for a few minutes each day - which can cause degraded performance as we go over our provisioned vCPUs.
* My reading is that you can setup Aurora Serverless v2 to sit as a reader *alongside* regular RI readers, so traffic can go to both types at the same time, and this will be handled by RDS Proxy.

So the core assumption I'm looking to validate: **can I use RIs to cut costs for our baseline usage and use Aurora Serverless v2 to sit on standby to handle spikiness?** I realise there might be an obvious answer based on my last bullet point above, but every blog post I'm reading always compares the two services as an either/or option, and typically concludes that Aurora Serverless v2 ends up simply being more expensive because it doesn't make sense for baseline usage. I haven't found any posts of people talking about this setup.

Follow on questions that I'm also not getting clear answers on:

* Is RDS Proxy intelligent enough when you're using readers and serverless together (i.e. will it start sending spiky traffic to serverless, rather than splitting amongst all readers?)
* Can our apu be left at 0.5 and it will autoscale to any capacity we need, or should we try and optimise our apu minimum to be equal to the memory our database typically needs? Is there a simple way of working this out?
* What are the other gotchas that we should be considering?

Final more unrelated question:

* We don't have an AWS support contract today. If we had one, would they be good at answering these types of questions, or are they more useful for just having someone to call when something goes wrong?",1708768936.0,5
aws,https://www.reddit.com/r/aws/comments/1ayplbk/lambda_function_authentication/,Lambda function authentication ,"Really new to all this stuff. I have a lambda function talking to OpenAI api which accessible via an endpoint (API gateway). This endpoint is being called from my react native app. 

The whole reason to create this function was because I did not want to store the api key in the app code. 

Now, I am facing issue with authenticating this endpoint. What simple yet secure enough solutions can I use to authenticate my endpoint? Another api key might be a solution but again it gets exposed client side",1708763157.0,6
aws,https://www.reddit.com/r/aws/comments/1aytl6f/lambdaedge_cold_start/,Lambda@Edge cold start?,"Hello

Is there any way to keep lambda @ edge warmed up? 

Otherwise, it ends up slowing down the website significantly... which is counter intuitive ",1708778364.0,1
aws,https://www.reddit.com/r/aws/comments/1aywm89/any_python_libraries_out_there_that_mock_the/,Any Python libraries out there that mock the MultiProcessing module and instead pass code to AWS Lambda to run?,"There are isolated functions / modules in my code that get invoked \~100k times (around 3 minutes for each invocation) each time I launch my code.  I can easily make this parallel using Python's multiprocessing module, but even with multiprocessing it'll take forever to run. Is there an interface similar to multiprocessing that pipes to lambda instead? The isolated functions are in heavy development, so I'm trying to avoid having to futz with aws-cli, AWS Console, IAM permissions, downloading from S3, etc... everytime I make a small change. I just want to run and have the results come back directly into Python.

Does such a thing exist?",1708787084.0,1
aws,https://www.reddit.com/r/aws/comments/1aywfuv/aws_policy_need_explaination_for_the_forallvalues/,"AWS policy: need explaination for the ""ForAllValues"" qualifier","I have this SCP attached to account A in my org

    {
      ""Version"": ""2012-10-17"",
      ""Statement"": [
        {
          ""Effect"": ""Deny"",
          ""Action"": [
            ""route53:ChangeResourceRecordSets""
          ],
          ""Resource"": ""*"",
          ""Condition"": {
            ""ForAllValues:StringEquals"": {
              ""route53:ChangeResourceRecordSetsRecordTypes"": [
                ""A"",
                ""AAAA""
              ]
            }
          }
        }
      ]
    }

As I understand:

* the SCP will deny if each and every record's type in my create-record request equals A or AAAA
* the SCP will not deny if at least one record's type in my create-record request is not equal A or AAAA

However, when I create R53 records in account A using the console:

https://preview.redd.it/kn6lkirqrjkc1.png?width=3360&format=png&auto=webp&s=e2de7f40ae3affcb39578ce271047ab4051bedf3

the SCP still blocks the request even though the request contains a record of type ""CNAME"" which is not A or AAAA

What is wrong in my understanding of this ""ForAllValues"" ? Please help me. Thanks",1708786626.0,1
aws,https://www.reddit.com/r/aws/comments/1ayrqmq/is_it_recommended_to_use_aws_to_host_docker/,is it recommended to use aws to host docker containers,"so I have a docker image that I have pushed to the docker hub registry. Now I have to host it somewhere and I was looking at some of the different options. AWS seems really good but i keep reading that they have a free tier but its not actually free and people being charged thousands of dollars unknowingly. If I were to start playing around with AWS today, how likely would it be that something like that would happen to me? Are there any other options I should try instead? thanks",1708771695.0,3
aws,https://www.reddit.com/r/aws/comments/1ay99so/bedrock_adds_mistral_ai_models/,Bedrock adds Mistral AI models,"even more AI models are being added to Bedrock:  


[https://www.aboutamazon.com/news/aws/mistral-ai-amazon-bedrock](https://www.aboutamazon.com/news/aws/mistral-ai-amazon-bedrock)",1708716383.0,20
aws,https://www.reddit.com/r/aws/comments/1ay9pon/is_anyone_using_codecommit_codebuild_codedeploy/,"Is anyone using CodeCommit, CodeBuild, CodeDeploy and CodePipeline?","I am currently studying for the AWS DevOps Professional certificate. On A Cloud Guru there is this section about CodeCommit, CodeBuild, CodeDeploy and CodePipeline. Why would I use any of those services if I can also use Github and Github Actions?",1708717463.0,15
aws,https://www.reddit.com/r/aws/comments/1ayykyl/aws_transcribe_is_crazy_expensive_how_to_lower/,AWS Transcribe is crazy expensive. How to lower costs and keep comparable performance?,"Looking for information on AWS hosted alternatives, e.g. Whisper on EC2. Has anyone switched over to this? If so how low are you able to get your costs to transcribe audio and video with comparable quality?",1708792054.0,17
aws,https://www.reddit.com/r/aws/comments/1azggur/one_vs_more_vpn/,One vs more VPN,"We are debating if se should use one or more Customer Gateway/VPN connections with our Transit Gateway. To me it looks like it just increases how much resources we need to maintain(more in Aws and onprem both), without adding much resiliency.

As I can tell our current CGW/connection have never been down, did AWS ever have outtage id this service?
And if it did, it would impact us the same no matter if we had 1 or 4 CGWs as they would all be affected.",1708840108.0,1
aws,https://www.reddit.com/r/aws/comments/1azburu/new_accounts_under_organisation_are_not/,New accounts under organisation are not consolidating billing,"I have a personal AWS org. The management account is setup with billing and so I can create resources within it. Obviously, I don't want to use the management account for that - but every time I create a new account in the org it is considered ""NotSignedUp"", so accessing S3 will 404, etc. 

As I understand it, any new/invited account within an org now, by default, consolidates its billing under the management account?

Does anyone know why new accounts might be treated as NotSignedUp?",1708825514.0,2
aws,https://www.reddit.com/r/aws/comments/1azfkay/mqtt_connection_failure_with_aws_iot_core/,MQTT connection failure with AWS IoT Core,"I need some assistance with a frustrating issue I'm encountering while connecting my device to AWS IoT Core. I'm using the \`aws-iot-device-sdk-v2\` library for JS and keep hitting this error:

`Error: aws-c-io: AWS_IO_TLS_ERROR_NEGOTIATION_FAILURE, TLS (SSL) negotiation failed`

This happens when I try to connect using `newWebsocketMqttBuilderWithSigv4Auth`

This code is from samples provided by AWS

Because other functions of the SDK aren't available for react-native.

    function createClientConfig(args: any): mqtt5.Mqtt5ClientConfig {
      let builder: iot.AwsIotMqtt5ClientConfigBuilder | undefined = undefined;
    
      let wsOptions: iot.WebsocketSigv4Config | undefined = undefined;
      if (args.region) {
        wsOptions = {
          region: args.region,
          // credentialsProvider: auth.AwsCredentialsProvider.newDefault(),
        };
      }
    
      builder =
        iot.AwsIotMqtt5ClientConfigBuilder.newWebsocketMqttBuilderWithSigv4Auth(
          args.endpoint,
          wsOptions
        );
      builder.withCertificateAuthorityFromPath(undefined, args.cert);
    
      builder.withConnectProperties({
        keepAliveIntervalSeconds: 1200,
      });
    
      return builder.build();
    }function createClientConfig(args: any): mqtt5.Mqtt5ClientConfig {
      let builder: iot.AwsIotMqtt5ClientConfigBuilder | undefined = undefined;
    
    
      let wsOptions: iot.WebsocketSigv4Config | undefined = undefined;
      if (args.region) {
        wsOptions = {
          region: args.region,
          // credentialsProvider: auth.AwsCredentialsProvider.newDefault(),
        };
      }
    
    
      builder =
        iot.AwsIotMqtt5ClientConfigBuilder.newWebsocketMqttBuilderWithSigv4Auth(
          args.endpoint,
          wsOptions
        );
      builder.withCertificateAuthorityFromPath(undefined, args.cert);
    
    
      builder.withConnectProperties({
        keepAliveIntervalSeconds: 1200,
      });
    
    
      return builder.build();
    }



`credentialsProvider: auth.AwsCredentialsProvider.newDefault()`

This line was suggested by GPT which didn't do shit.

I guess I need to add some sort of auth on the AWS side first and then use that to access the endpoint from here. But what and how?

The method below works fine when run in Node, but it's not available on react-native

    if (args.key && args.cert) {
     builder =
       iot.AwsIotMqtt5ClientConfigBuilder.newDirectMqttBuilderWithMtlsFromPath(
         args.endpoint,
         args.cert,
         args.key
       );

I've tried several things to resolve it, but I'm still scratching my head.

There's a few tutorials about slightly different things that talk about doing something with Cognito or Amplify but I couldn't really understand them (or didn't want to). I thought that maybe there was a simpler way

This is my first time dabbling in IoT, AWS and React Native.

I truly appreciate any insights or suggestions you can offer!",1708837081.0,1
aws,https://www.reddit.com/gallery/1azcz6i,Cloud Quest - API Gateway,"Can someone please help me with this one. I am now too invested in this game and I cant just ignore it. Hahaha ü•π

And I got this result though I did everything. I already spent 1hour configuring it but still the result was ""AN ERROR (403) OCCURED WHILE INVOKING API WITH THE URL YOU PROVIDED.""",1708828822.0,0
aws,https://www.reddit.com/r/aws/comments/1aza0fo/java_shared_code_in_a_lambda_layer_how_will/,[Java] Shared code in a lambda layer. How will lambdas access this layer?,"I have 5+ lambdas. Each lambda does its work by using 1 specific class and multiple utility classes. All the lambdas live in the same repository. Their `build.gradle`s all have the same dependencies. Small example:

    CreateLambda
      src
       main
        CreateHandler.java
        CommonUtility1.java
        CommonUtility2.java
      build.gradle

    UpdateLambda
      src
       main
        UpdateHandler.java
        CommonUtility1.java
        CommonUtility2.java
      build.gradle

This results in identical utility classes being duplicated everywhere. Therefore I am looking into creating a `lambda layer` that will contain all the common dependencies and shared utility classes that the 3 lambdas will use. Small updated example:

    LambdaLayer
      src
       main
        CommonUtility1.java
        CommonUtility2.java
      build.gradle

    CreateLambda
      src
       main
        CreateHandler.java
      build.gradle

    UpdateLambda
      src
       main
        UpdateHandler.java
      build.gradle

My question is: **How would the lambdas import the lambda layer as a dependency?** Would they just have to import it as a dependency in the `build.gradle` as they would with any other dependency? That would mean that the lambda layer JAR would have to be published first before the lambdas can use it right? This is for work and at my company all JARs get published to a company maven repository.

Anything else I need to know in order to do this properly? Lambda layer tutorials in Java are rare when I google",1708820419.0,1
aws,https://www.reddit.com/r/aws/comments/1az9s0s/video_on_demand_via_aws_media_convert/,Video on Demand via AWS Media Convert?,"Hello, using S3 and cloudfront for video on demand services(50mb - 5gb files), however data transfer costs are getting out of hand;

1. Is media convert a suitable approach(cost, overhead, efficacy, etc)?
2. Does media convert have adaptive bitrate streaming?
3. Id like to load videos in ""chunks"" instead of loading entire file like i am currently, does media convert help with this?",1708819813.0,1
aws,https://www.reddit.com/r/aws/comments/1az94ga/video_on_demand_options/,Video On Demand options?,"Currently storing via S3 and serving via CloudFront, however the data transfer costs are still too high and id like to find a solution:

1. Should I even be using s3 + CloudFront for this?
2. What is adaptive bitrate? What are my options for this?
3. Should I convert/compress videos upon upload? If so what are my options
4. Proper CloudFront utlization? Currently im signing cloudfront URLs and sending it directly to client, does cloudfront automatically cache and compress files or must I do something else?",1708818106.0,0
aws,https://www.reddit.com/r/aws/comments/1az8hwb/is_it_possible_to_grant_a_lightsail_instance/,Is it possible to grant a Lightsail instance access to Bedrock via Service Roles?,"I have a Lighsail instance that I would like to make requests to my Bedrock FM. I could create Access Key/Access Key Secret, but I am wondering if this is possible via Service Roles. It seems Lightsail uses a Service Linked Role, which cannot be modified. I just want to make sure I am understanding that correctly.

[https://docs.aws.amazon.com/IAM/latest/UserGuide/reference\_aws-services-that-work-with-iam.html#all\_svcs](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html#all_svcs)",1708816473.0,1
aws,https://www.reddit.com/r/aws/comments/1ayy7hq/aws_cognito_giving_me_code_that_violates_their/,AWS Cognito giving me code that violates their own policy?,"I am creating a website using AWS S3 and I am using Cognito for user accounts.  


I have Cognito set up and working on my website to where a user can sign up for an account. However, whenever a user is directed to the link provided by Cognito for authentication (ex: https://<website name>-login.auth.us-east-1.amazoncognito.com/login?client\_id=<client id>&response\_type=code&scope=email+openid&redirect\_uri=https%3A%2F%2F<domain name>%2F<website\_name>.html), the following error is had in the console:  


""Refused to execute inline script because it violates the following Content Security Policy directive: ""script-src [https://d3oia8etllorh5.cloudfront.net](https://d3oia8etllorh5.cloudfront.net) https://<website name>-login.auth.us-east-1.amazoncognito.com"". Either the 'unsafe-inline' keyword, a hash ('sha256-fWEffNYGDN+XQ+YrsU/LKDZAnOcnSlra1fOIm+7oskM='), or a nonce ('nonce-...') is required to enable inline execution.""  


Basically, it is saying that on this authentication website provided by Cognito, it uses an inline script, which is not allowed by AWS' Content Security Policy. When opening up the code for the authentication website via ""Inspect"", there is in fact inline script.  


How come AWS gave me code that violates their own CSP Policy? How do I get around this? I've tried many things, but nothing seems to work.  


Thanks for any help!",1708791127.0,4
aws,https://www.reddit.com/r/aws/comments/1aym3nw/how_do_you_implement_platform_engineering/,How do you implement platform engineering??,"Okay, I‚Äôm working as a sr ‚Äúdevops‚Äù engineer with a software developer background trying to build a platform for a client. I‚Äôll try to keep my opinions out of it, but I don‚Äôt love platform engineering and I don‚Äôt understand how it could possibly scale‚Ä¶at least not with what we have built. 

Some context, we are using a gitops approach for deploying infrastructure onto aws. We use Kubernetes based terraform operator (yeah questionable‚Ä¶I know) and ArgoCD to manage deployments of infra. 

We created several terraform modules that contain a SINGLE aws resource in its own git repository. There are some ‚Äúsensible defaults‚Äù in the modules and a bunch of variables for users to input if they choose or not. Tons of conditional logic in the templates. 

Our plan is to enable these to be consumed through an IDP (internal developer portal) to give devs an easy button. 

My question is, how does this scale. It‚Äôs very challenging to write single modules that can be deployed with their own individual terraform state. So I can‚Äôt reference outputs and bind resources together very easily without multi step deployments sometimes. Or guessing at what the output name of a resource might be. 

For example, it‚Äôs very hard to do this with a native aws cloud solution like s3 bucket that triggers lambda based on putObject that then sends a message to sqs and is consumed by another lambda. Or triggering a lambda based on RDS input etc etc. 

So, my question is how do you make a ‚Äúplatform/product‚Äù that allows for flexibility for product teams and devs to consume services through a UI or some easy button without writing the terraform themselves?? 

TL;DR: 
How do you write terraform modules in a platform? ",1708750638.0,19
aws,https://www.reddit.com/r/aws/comments/1az3tm4/nvidia_driver_for_g5xlarge/,Nvidia driver for g5.xlarge,"Hello,

I just take a g5.xlarge  with an Amazon Linux 2023 AMI and I struggle to install an nvidia driver,  so please I have some questions:

How can I install an nvida driver there ? I konw there is some AMI which comes already with a driver but I don't want thid solution

Which driver should I use ? My purpose is to play with llama2 to do some GenAI (RAG)

Are nvidia drivers free ?

Thanks a lot
",1708804929.0,0
aws,https://www.reddit.com/r/aws/comments/1az31m3/aws_cli_how_can_i_filter_a_file_list_from_the_ls/,[AWS CLI] how can i filter a file list from the ls command?,"Is there a way to filter a file listing from the ls command in the AWS CLI? e.g. What would I have to do (from the command line) if I just want to see or download a list of all the zip files in a folder that has over a million files where only some are zips? I tried:  
  
aws s3 ls s3://(bucket)/(folder) --exclude ""*"" --include ""*.zip""  
  
but that doesn't work - surely there must be some way to do this? I guess it'd take the same amount of time to list the results either way but it'd be nice to be able to get a filtered result (and a smaller text output file) after the wait rather than having to do manually filter that for what i want afterwards.",1708802953.0,1
aws,https://www.reddit.com/r/aws/comments/1ayxgg5/questions_on_org_trail_in_control_tower/,Question(s) on Org Trail in Control Tower,"Hello,

I would appreciate if some kind soul could give me pointers on what I am trying to achieve. I may not be using the correct search terms when looking around the interwebs.

 We are getting started with our AWS journey with Control Tower being used to come up with a well architected framework as recommended by AWS.

The one thing I am a bit confused about is, how do we monitor all the CloudTrail events in the ""Audit"" account with our own custom alert.  The Control Tower framework has created the OrgTrail with the Audit account having access to all accounts events, I see AWS Guard Duty monitoring and occasionally alerting me on stuff.  

Q1: How do I extend the alerting above and beyond what AWS Guard Duty does?  

Q2: We are comfortable with our on-prem SIEM and although I am aware of the costs involved in bringing in CloudTrail events through our OrgTrail, it is something we are comfortable with to get started.  How do I do this?  I am assuming this is possible.  

Thank you all!

GT

&#x200B;",1708789240.0,2
aws,https://www.reddit.com/r/aws/comments/1az2jsb/deployment/,Deployment,"I need yalls help!! 

I've never used aws before and i have a question regarding the deployment of a website. 

Im working on a school project and the topic is gitlab ci/cd pipeline. I've developed my own web app using flask and angular and i have two separate folders for each of those. The app is for demonstrational purposes only, nobody would actually be using it. How do i deploy that? Ive read that for backend i could probably use beanstalk? What about the frontend? Do you make two separate jobs in a pipeline for backend and frontend? How do they communicate in the cloud? 

Lol, i have so many questions and no idea where to start. Any courses/literature recommendations are welcomed. ",1708801726.0,0
aws,https://www.reddit.com/r/aws/comments/1ayr108/combining_aurora_serverless_v2_with_reserved/,Combining Aurora Serverless v2 with Reserved Instances,"Hey everyone, I'd like to validate an approach before diving a rabbit hole of setup:

* We have a fairly standard baseline use to our Aurora databases, so we use a couple r7g.large instances in each of the major regions we operate. This is great 99.9% of the time.
* However, within each region things can get spikey - often only for a few minutes each day - which can cause degraded performance as we go over our provisioned vCPUs.
* My reading is that you can setup Aurora Serverless v2 to sit as a reader *alongside* regular RI readers, so traffic can go to both types at the same time, and this will be handled by RDS Proxy.

So the core assumption I'm looking to validate: **can I use RIs to cut costs for our baseline usage and use Aurora Serverless v2 to sit on standby to handle spikiness?** I realise there might be an obvious answer based on my last bullet point above, but every blog post I'm reading always compares the two services as an either/or option, and typically concludes that Aurora Serverless v2 ends up simply being more expensive because it doesn't make sense for baseline usage. I haven't found any posts of people talking about this setup.

Follow on questions that I'm also not getting clear answers on:

* Is RDS Proxy intelligent enough when you're using readers and serverless together (i.e. will it start sending spiky traffic to serverless, rather than splitting amongst all readers?)
* Can our apu be left at 0.5 and it will autoscale to any capacity we need, or should we try and optimise our apu minimum to be equal to the memory our database typically needs? Is there a simple way of working this out?
* What are the other gotchas that we should be considering?

Final more unrelated question:

* We don't have an AWS support contract today. If we had one, would they be good at answering these types of questions, or are they more useful for just having someone to call when something goes wrong?",1708768936.0,5
aws,https://www.reddit.com/r/aws/comments/1ayplbk/lambda_function_authentication/,Lambda function authentication ,"Really new to all this stuff. I have a lambda function talking to OpenAI api which accessible via an endpoint (API gateway). This endpoint is being called from my react native app. 

The whole reason to create this function was because I did not want to store the api key in the app code. 

Now, I am facing issue with authenticating this endpoint. What simple yet secure enough solutions can I use to authenticate my endpoint? Another api key might be a solution but again it gets exposed client side",1708763157.0,7
aws,https://www.reddit.com/r/aws/comments/1aytl6f/lambdaedge_cold_start/,Lambda@Edge cold start?,"Hello

Is there any way to keep lambda @ edge warmed up? 

Otherwise, it ends up slowing down the website significantly... which is counter intuitive ",1708778364.0,3
aws,https://www.reddit.com/r/aws/comments/1aywm89/any_python_libraries_out_there_that_mock_the/,Any Python libraries out there that mock the MultiProcessing module and instead pass code to AWS Lambda to run?,"There are isolated functions / modules in my code that get invoked \~100k times (around 3 minutes for each invocation) each time I launch my code.  I can easily make this parallel using Python's multiprocessing module, but even with multiprocessing it'll take forever to run. Is there an interface similar to multiprocessing that pipes to lambda instead? The isolated functions are in heavy development, so I'm trying to avoid having to futz with aws-cli, AWS Console, IAM permissions, downloading from S3, etc... everytime I make a small change. I just want to run and have the results come back directly into Python.

Does such a thing exist?",1708787084.0,1
aws,https://www.reddit.com/r/aws/comments/1aywfuv/aws_policy_need_explaination_for_the_forallvalues/,"AWS policy: need explaination for the ""ForAllValues"" qualifier","I have this SCP attached to account A in my org

    {
      ""Version"": ""2012-10-17"",
      ""Statement"": [
        {
          ""Effect"": ""Deny"",
          ""Action"": [
            ""route53:ChangeResourceRecordSets""
          ],
          ""Resource"": ""*"",
          ""Condition"": {
            ""ForAllValues:StringEquals"": {
              ""route53:ChangeResourceRecordSetsRecordTypes"": [
                ""A"",
                ""AAAA""
              ]
            }
          }
        }
      ]
    }

As I understand:

* the SCP will deny if each and every record's type in my create-record request equals A or AAAA
* the SCP will not deny if at least one record's type in my create-record request is not equal A or AAAA

However, when I create R53 records in account A using the console:

https://preview.redd.it/kn6lkirqrjkc1.png?width=3360&format=png&auto=webp&s=e2de7f40ae3affcb39578ce271047ab4051bedf3

the SCP still blocks the request even though the request contains a record of type ""CNAME"" which is not A or AAAA

What is wrong in my understanding of this ""ForAllValues"" ? Please help me. Thanks",1708786626.0,1
aws,https://www.reddit.com/r/aws/comments/1ayrqmq/is_it_recommended_to_use_aws_to_host_docker/,is it recommended to use aws to host docker containers,"so I have a docker image that I have pushed to the docker hub registry. Now I have to host it somewhere and I was looking at some of the different options. AWS seems really good but i keep reading that they have a free tier but its not actually free and people being charged thousands of dollars unknowingly. If I were to start playing around with AWS today, how likely would it be that something like that would happen to me? Are there any other options I should try instead? thanks",1708771695.0,1
aws,https://www.reddit.com/r/aws/comments/1ay99so/bedrock_adds_mistral_ai_models/,Bedrock adds Mistral AI models,"even more AI models are being added to Bedrock:  


[https://www.aboutamazon.com/news/aws/mistral-ai-amazon-bedrock](https://www.aboutamazon.com/news/aws/mistral-ai-amazon-bedrock)",1708716383.0,21
aws,https://www.reddit.com/r/aws/comments/1ay9pon/is_anyone_using_codecommit_codebuild_codedeploy/,"Is anyone using CodeCommit, CodeBuild, CodeDeploy and CodePipeline?","I am currently studying for the AWS DevOps Professional certificate. On A Cloud Guru there is this section about CodeCommit, CodeBuild, CodeDeploy and CodePipeline. Why would I use any of those services if I can also use Github and Github Actions?",1708717463.0,16
aws,https://www.reddit.com/r/aws/comments/1ayykyl/aws_transcribe_is_crazy_expensive_how_to_lower/,AWS Transcribe is crazy expensive. How to lower costs and keep comparable performance?,"Looking for information on AWS hosted alternatives, e.g. Whisper on EC2. Has anyone switched over to this? If so how low are you able to get your costs to transcribe audio and video with comparable quality?",1708792054.0,18
aws,https://www.reddit.com/r/aws/comments/1azhazj/awssecretinject/,aws-secret-inject,"Hi,

I made a quick little tool that might help others.  

https://www.npmjs.com/package/aws-secret-inject

My team would typically create local .env files inside 1Password or LastPass.  It always created multiple places to keep in sync which caused headaches. 

Now the team can checkin to source control some kind of local.env.template that a new person could generate a .env file from.  

I wrote the tool text file agnostic, so it works for any text file , not just .env

It‚Äôs going to be useful for my team, figured I would share in case it‚Äôs helpful for others. 
",1708843117.0,1
aws,https://www.reddit.com/r/aws/comments/1azggur/one_vs_more_vpn/,One vs more VPN,"We are debating if se should use one or more Customer Gateway/VPN connections with our Transit Gateway. To me it looks like it just increases how much resources we need to maintain(more in Aws and onprem both), without adding much resiliency.

As I can tell our current CGW/connection have never been down, did AWS ever have outtage id this service?
And if it did, it would impact us the same no matter if we had 1 or 4 CGWs as they would all be affected.",1708840108.0,1
aws,https://www.reddit.com/r/aws/comments/1azburu/new_accounts_under_organisation_are_not/,New accounts under organisation are not consolidating billing,"I have a personal AWS org. The management account is setup with billing and so I can create resources within it. Obviously, I don't want to use the management account for that - but every time I create a new account in the org it is considered ""NotSignedUp"", so accessing S3 will 404, etc. 

As I understand it, any new/invited account within an org now, by default, consolidates its billing under the management account?

Does anyone know why new accounts might be treated as NotSignedUp?",1708825514.0,2
aws,https://www.reddit.com/r/aws/comments/1azfkay/mqtt_connection_failure_with_aws_iot_core/,MQTT connection failure with AWS IoT Core,"I need some assistance with a frustrating issue I'm encountering while connecting my device to AWS IoT Core. I'm using the \`aws-iot-device-sdk-v2\` library for JS and keep hitting this error:

`Error: aws-c-io: AWS_IO_TLS_ERROR_NEGOTIATION_FAILURE, TLS (SSL) negotiation failed`

This happens when I try to connect using `newWebsocketMqttBuilderWithSigv4Auth`

This code is from samples provided by AWS

Because other functions of the SDK aren't available for react-native.

    function createClientConfig(args: any): mqtt5.Mqtt5ClientConfig {
      let builder: iot.AwsIotMqtt5ClientConfigBuilder | undefined = undefined;
    
      let wsOptions: iot.WebsocketSigv4Config | undefined = undefined;
      if (args.region) {
        wsOptions = {
          region: args.region,
          // credentialsProvider: auth.AwsCredentialsProvider.newDefault(),
        };
      }
    
      builder =
        iot.AwsIotMqtt5ClientConfigBuilder.newWebsocketMqttBuilderWithSigv4Auth(
          args.endpoint,
          wsOptions
        );
      builder.withCertificateAuthorityFromPath(undefined, args.cert);
    
      builder.withConnectProperties({
        keepAliveIntervalSeconds: 1200,
      });
    
      return builder.build();
    }function createClientConfig(args: any): mqtt5.Mqtt5ClientConfig {
      let builder: iot.AwsIotMqtt5ClientConfigBuilder | undefined = undefined;
    
    
      let wsOptions: iot.WebsocketSigv4Config | undefined = undefined;
      if (args.region) {
        wsOptions = {
          region: args.region,
          // credentialsProvider: auth.AwsCredentialsProvider.newDefault(),
        };
      }
    
    
      builder =
        iot.AwsIotMqtt5ClientConfigBuilder.newWebsocketMqttBuilderWithSigv4Auth(
          args.endpoint,
          wsOptions
        );
      builder.withCertificateAuthorityFromPath(undefined, args.cert);
    
    
      builder.withConnectProperties({
        keepAliveIntervalSeconds: 1200,
      });
    
    
      return builder.build();
    }



`credentialsProvider: auth.AwsCredentialsProvider.newDefault()`

This line was suggested by GPT which didn't do shit.

I guess I need to add some sort of auth on the AWS side first and then use that to access the endpoint from here. But what and how?

The method below works fine when run in Node, but it's not available on react-native

    if (args.key && args.cert) {
     builder =
       iot.AwsIotMqtt5ClientConfigBuilder.newDirectMqttBuilderWithMtlsFromPath(
         args.endpoint,
         args.cert,
         args.key
       );

I've tried several things to resolve it, but I'm still scratching my head.

There's a few tutorials about slightly different things that talk about doing something with Cognito or Amplify but I couldn't really understand them (or didn't want to). I thought that maybe there was a simpler way

This is my first time dabbling in IoT, AWS and React Native.

I truly appreciate any insights or suggestions you can offer!",1708837081.0,1
aws,https://www.reddit.com/gallery/1azcz6i,Cloud Quest - API Gateway,"Can someone please help me with this one. I am now too invested in this game and I cant just ignore it. Hahaha ü•π

And I got this result though I did everything. I already spent 1hour configuring it but still the result was ""AN ERROR (403) OCCURED WHILE INVOKING API WITH THE URL YOU PROVIDED.""",1708828822.0,0
aws,https://www.reddit.com/r/aws/comments/1aza0fo/java_shared_code_in_a_lambda_layer_how_will/,[Java] Shared code in a lambda layer. How will lambdas access this layer?,"I have 5+ lambdas. Each lambda does its work by using 1 specific class and multiple utility classes. All the lambdas live in the same repository. Their `build.gradle`s all have the same dependencies. Small example:

    CreateLambda
      src
       main
        CreateHandler.java
        CommonUtility1.java
        CommonUtility2.java
      build.gradle

    UpdateLambda
      src
       main
        UpdateHandler.java
        CommonUtility1.java
        CommonUtility2.java
      build.gradle

This results in identical utility classes being duplicated everywhere. Therefore I am looking into creating a `lambda layer` that will contain all the common dependencies and shared utility classes that the 3 lambdas will use. Small updated example:

    LambdaLayer
      src
       main
        CommonUtility1.java
        CommonUtility2.java
      build.gradle

    CreateLambda
      src
       main
        CreateHandler.java
      build.gradle

    UpdateLambda
      src
       main
        UpdateHandler.java
      build.gradle

My question is: **How would the lambdas import the lambda layer as a dependency?** Would they just have to import it as a dependency in the `build.gradle` as they would with any other dependency? That would mean that the lambda layer JAR would have to be published first before the lambdas can use it right? This is for work and at my company all JARs get published to a company maven repository.

Anything else I need to know in order to do this properly? Lambda layer tutorials in Java are rare when I google",1708820419.0,1
aws,https://www.reddit.com/r/aws/comments/1az9s0s/video_on_demand_via_aws_media_convert/,Video on Demand via AWS Media Convert?,"Hello, using S3 and cloudfront for video on demand services(50mb - 5gb files), however data transfer costs are getting out of hand;

1. Is media convert a suitable approach(cost, overhead, efficacy, etc)?
2. Does media convert have adaptive bitrate streaming?
3. Id like to load videos in ""chunks"" instead of loading entire file like i am currently, does media convert help with this?",1708819813.0,1
aws,https://www.reddit.com/r/aws/comments/1az94ga/video_on_demand_options/,Video On Demand options?,"Currently storing via S3 and serving via CloudFront, however the data transfer costs are still too high and id like to find a solution:

1. Should I even be using s3 + CloudFront for this?
2. What is adaptive bitrate? What are my options for this?
3. Should I convert/compress videos upon upload? If so what are my options
4. Proper CloudFront utlization? Currently im signing cloudfront URLs and sending it directly to client, does cloudfront automatically cache and compress files or must I do something else?",1708818106.0,0
aws,https://www.reddit.com/r/aws/comments/1az8hwb/is_it_possible_to_grant_a_lightsail_instance/,Is it possible to grant a Lightsail instance access to Bedrock via Service Roles?,"I have a Lighsail instance that I would like to make requests to my Bedrock FM. I could create Access Key/Access Key Secret, but I am wondering if this is possible via Service Roles. It seems Lightsail uses a Service Linked Role, which cannot be modified. I just want to make sure I am understanding that correctly.

[https://docs.aws.amazon.com/IAM/latest/UserGuide/reference\_aws-services-that-work-with-iam.html#all\_svcs](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-services-that-work-with-iam.html#all_svcs)",1708816473.0,1
aws,https://www.reddit.com/r/aws/comments/1ayy7hq/aws_cognito_giving_me_code_that_violates_their/,AWS Cognito giving me code that violates their own policy?,"I am creating a website using AWS S3 and I am using Cognito for user accounts.  


I have Cognito set up and working on my website to where a user can sign up for an account. However, whenever a user is directed to the link provided by Cognito for authentication (ex: https://<website name>-login.auth.us-east-1.amazoncognito.com/login?client\_id=<client id>&response\_type=code&scope=email+openid&redirect\_uri=https%3A%2F%2F<domain name>%2F<website\_name>.html), the following error is had in the console:  


""Refused to execute inline script because it violates the following Content Security Policy directive: ""script-src [https://d3oia8etllorh5.cloudfront.net](https://d3oia8etllorh5.cloudfront.net) https://<website name>-login.auth.us-east-1.amazoncognito.com"". Either the 'unsafe-inline' keyword, a hash ('sha256-fWEffNYGDN+XQ+YrsU/LKDZAnOcnSlra1fOIm+7oskM='), or a nonce ('nonce-...') is required to enable inline execution.""  


Basically, it is saying that on this authentication website provided by Cognito, it uses an inline script, which is not allowed by AWS' Content Security Policy. When opening up the code for the authentication website via ""Inspect"", there is in fact inline script.  


How come AWS gave me code that violates their own CSP Policy? How do I get around this? I've tried many things, but nothing seems to work.  


Thanks for any help!",1708791127.0,2
aws,https://www.reddit.com/r/aws/comments/1aym3nw/how_do_you_implement_platform_engineering/,How do you implement platform engineering??,"Okay, I‚Äôm working as a sr ‚Äúdevops‚Äù engineer with a software developer background trying to build a platform for a client. I‚Äôll try to keep my opinions out of it, but I don‚Äôt love platform engineering and I don‚Äôt understand how it could possibly scale‚Ä¶at least not with what we have built. 

Some context, we are using a gitops approach for deploying infrastructure onto aws. We use Kubernetes based terraform operator (yeah questionable‚Ä¶I know) and ArgoCD to manage deployments of infra. 

We created several terraform modules that contain a SINGLE aws resource in its own git repository. There are some ‚Äúsensible defaults‚Äù in the modules and a bunch of variables for users to input if they choose or not. Tons of conditional logic in the templates. 

Our plan is to enable these to be consumed through an IDP (internal developer portal) to give devs an easy button. 

My question is, how does this scale. It‚Äôs very challenging to write single modules that can be deployed with their own individual terraform state. So I can‚Äôt reference outputs and bind resources together very easily without multi step deployments sometimes. Or guessing at what the output name of a resource might be. 

For example, it‚Äôs very hard to do this with a native aws cloud solution like s3 bucket that triggers lambda based on putObject that then sends a message to sqs and is consumed by another lambda. Or triggering a lambda based on RDS input etc etc. 

So, my question is how do you make a ‚Äúplatform/product‚Äù that allows for flexibility for product teams and devs to consume services through a UI or some easy button without writing the terraform themselves?? 

TL;DR: 
How do you write terraform modules in a platform? ",1708750638.0,21
aws,https://www.reddit.com/r/aws/comments/1az3tm4/nvidia_driver_for_g5xlarge/,Nvidia driver for g5.xlarge,"Hello,

I just take a g5.xlarge  with an Amazon Linux 2023 AMI and I struggle to install an nvidia driver,  so please I have some questions:

How can I install an nvida driver there ? I konw there is some AMI which comes already with a driver but I don't want thid solution

Which driver should I use ? My purpose is to play with llama2 to do some GenAI (RAG)

Are nvidia drivers free ?

Thanks a lot
",1708804929.0,0
aws,https://www.reddit.com/r/aws/comments/1az31m3/aws_cli_how_can_i_filter_a_file_list_from_the_ls/,[AWS CLI] how can i filter a file list from the ls command?,"Is there a way to filter a file listing from the ls command in the AWS CLI? e.g. What would I have to do (from the command line) if I just want to see or download a list of all the zip files in a folder that has over a million files where only some are zips? I tried:  
  
aws s3 ls s3://(bucket)/(folder) --exclude ""*"" --include ""*.zip""  
  
but that doesn't work - surely there must be some way to do this? I guess it'd take the same amount of time to list the results either way but it'd be nice to be able to get a filtered result (and a smaller text output file) after the wait rather than having to do manually filter that for what i want afterwards.",1708802953.0,1
aws,https://www.reddit.com/r/aws/comments/1ayxgg5/questions_on_org_trail_in_control_tower/,Question(s) on Org Trail in Control Tower,"Hello,

I would appreciate if some kind soul could give me pointers on what I am trying to achieve. I may not be using the correct search terms when looking around the interwebs.

 We are getting started with our AWS journey with Control Tower being used to come up with a well architected framework as recommended by AWS.

The one thing I am a bit confused about is, how do we monitor all the CloudTrail events in the ""Audit"" account with our own custom alert.  The Control Tower framework has created the OrgTrail with the Audit account having access to all accounts events, I see AWS Guard Duty monitoring and occasionally alerting me on stuff.  

Q1: How do I extend the alerting above and beyond what AWS Guard Duty does?  

Q2: We are comfortable with our on-prem SIEM and although I am aware of the costs involved in bringing in CloudTrail events through our OrgTrail, it is something we are comfortable with to get started.  How do I do this?  I am assuming this is possible.  

Thank you all!

GT

&#x200B;",1708789240.0,2
aws,https://www.reddit.com/r/aws/comments/1az2jsb/deployment/,Deployment,"I need yalls help!! 

I've never used aws before and i have a question regarding the deployment of a website. 

Im working on a school project and the topic is gitlab ci/cd pipeline. I've developed my own web app using flask and angular and i have two separate folders for each of those. The app is for demonstrational purposes only, nobody would actually be using it. How do i deploy that? Ive read that for backend i could probably use beanstalk? What about the frontend? Do you make two separate jobs in a pipeline for backend and frontend? How do they communicate in the cloud? 

Lol, i have so many questions and no idea where to start. Any courses/literature recommendations are welcomed. ",1708801726.0,0
aws,https://www.reddit.com/r/aws/comments/1ayr108/combining_aurora_serverless_v2_with_reserved/,Combining Aurora Serverless v2 with Reserved Instances,"Hey everyone, I'd like to validate an approach before diving a rabbit hole of setup:

* We have a fairly standard baseline use to our Aurora databases, so we use a couple r7g.large instances in each of the major regions we operate. This is great 99.9% of the time.
* However, within each region things can get spikey - often only for a few minutes each day - which can cause degraded performance as we go over our provisioned vCPUs.
* My reading is that you can setup Aurora Serverless v2 to sit as a reader *alongside* regular RI readers, so traffic can go to both types at the same time, and this will be handled by RDS Proxy.

So the core assumption I'm looking to validate: **can I use RIs to cut costs for our baseline usage and use Aurora Serverless v2 to sit on standby to handle spikiness?** I realise there might be an obvious answer based on my last bullet point above, but every blog post I'm reading always compares the two services as an either/or option, and typically concludes that Aurora Serverless v2 ends up simply being more expensive because it doesn't make sense for baseline usage. I haven't found any posts of people talking about this setup.

Follow on questions that I'm also not getting clear answers on:

* Is RDS Proxy intelligent enough when you're using readers and serverless together (i.e. will it start sending spiky traffic to serverless, rather than splitting amongst all readers?)
* Can our apu be left at 0.5 and it will autoscale to any capacity we need, or should we try and optimise our apu minimum to be equal to the memory our database typically needs? Is there a simple way of working this out?
* What are the other gotchas that we should be considering?

Final more unrelated question:

* We don't have an AWS support contract today. If we had one, would they be good at answering these types of questions, or are they more useful for just having someone to call when something goes wrong?",1708768936.0,5
aws,https://www.reddit.com/r/aws/comments/1ayplbk/lambda_function_authentication/,Lambda function authentication ,"Really new to all this stuff. I have a lambda function talking to OpenAI api which accessible via an endpoint (API gateway). This endpoint is being called from my react native app. 

The whole reason to create this function was because I did not want to store the api key in the app code. 

Now, I am facing issue with authenticating this endpoint. What simple yet secure enough solutions can I use to authenticate my endpoint? Another api key might be a solution but again it gets exposed client side",1708763157.0,6
aws,https://www.reddit.com/r/aws/comments/1aytl6f/lambdaedge_cold_start/,Lambda@Edge cold start?,"Hello

Is there any way to keep lambda @ edge warmed up? 

Otherwise, it ends up slowing down the website significantly... which is counter intuitive ",1708778364.0,3
aws,https://www.reddit.com/r/aws/comments/1aywm89/any_python_libraries_out_there_that_mock_the/,Any Python libraries out there that mock the MultiProcessing module and instead pass code to AWS Lambda to run?,"There are isolated functions / modules in my code that get invoked \~100k times (around 3 minutes for each invocation) each time I launch my code.  I can easily make this parallel using Python's multiprocessing module, but even with multiprocessing it'll take forever to run. Is there an interface similar to multiprocessing that pipes to lambda instead? The isolated functions are in heavy development, so I'm trying to avoid having to futz with aws-cli, AWS Console, IAM permissions, downloading from S3, etc... everytime I make a small change. I just want to run and have the results come back directly into Python.

Does such a thing exist?",1708787084.0,1
aws,https://www.reddit.com/r/aws/comments/1aywfuv/aws_policy_need_explaination_for_the_forallvalues/,"AWS policy: need explaination for the ""ForAllValues"" qualifier","I have this SCP attached to account A in my org

    {
      ""Version"": ""2012-10-17"",
      ""Statement"": [
        {
          ""Effect"": ""Deny"",
          ""Action"": [
            ""route53:ChangeResourceRecordSets""
          ],
          ""Resource"": ""*"",
          ""Condition"": {
            ""ForAllValues:StringEquals"": {
              ""route53:ChangeResourceRecordSetsRecordTypes"": [
                ""A"",
                ""AAAA""
              ]
            }
          }
        }
      ]
    }

As I understand:

* the SCP will deny if each and every record's type in my create-record request equals A or AAAA
* the SCP will not deny if at least one record's type in my create-record request is not equal A or AAAA

However, when I create R53 records in account A using the console:

https://preview.redd.it/kn6lkirqrjkc1.png?width=3360&format=png&auto=webp&s=e2de7f40ae3affcb39578ce271047ab4051bedf3

the SCP still blocks the request even though the request contains a record of type ""CNAME"" which is not A or AAAA

What is wrong in my understanding of this ""ForAllValues"" ? Please help me. Thanks",1708786626.0,1
aws,https://www.reddit.com/r/aws/comments/1ayrqmq/is_it_recommended_to_use_aws_to_host_docker/,is it recommended to use aws to host docker containers,"so I have a docker image that I have pushed to the docker hub registry. Now I have to host it somewhere and I was looking at some of the different options. AWS seems really good but i keep reading that they have a free tier but its not actually free and people being charged thousands of dollars unknowingly. If I were to start playing around with AWS today, how likely would it be that something like that would happen to me? Are there any other options I should try instead? thanks",1708771695.0,2
aws,https://www.reddit.com/r/aws/comments/1ay99so/bedrock_adds_mistral_ai_models/,Bedrock adds Mistral AI models,"even more AI models are being added to Bedrock:  


[https://www.aboutamazon.com/news/aws/mistral-ai-amazon-bedrock](https://www.aboutamazon.com/news/aws/mistral-ai-amazon-bedrock)",1708716383.0,21
=======
&#x200B;",1708639575.0,1
swiggy,https://www.reddit.com/r/aws/comments/1axddzy/aws_marketplace_publishing_a_product_is_taking_so/,AWS Marketplace: publishing a product is taking SO long,"Has anyone been able to publish their products (AMI, SaaS, etc) to AWS marketplace recently? My team has tried to publish a few things since like August 2023 but I think none of the products went through the review process, its been stuck on a request to change from limited visibility to public since then.

Any ideas or information will be useful, its been very frustrating.",1708625773.0,2
>>>>>>> main
